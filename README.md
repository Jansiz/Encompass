<h1> ENCOMPASS </h1>

<h3>check out Encompass: <a href="www.aiencompass.com">aiencompass.com</a></h3>
<h2> System Architecture Overview </h2>
<br>

![encompass-prototype-systemdesign](https://github.com/user-attachments/assets/6bd64bc7-22a6-498d-abd5-ccfee21dc2a9)

<p>This document provides a detailed description of the system architecture for our application, which involves user interaction, data processing using a Large Language Model (LLM), video generation LLM, and efficient data storage.</p>

<h2>1. Frontend</h2>
<h3>Components:</h3>
<ul>
    <li><strong>Load Balancer:</strong> The entry point for all user requests. It distributes incoming traffic across multiple frontend servers to ensure no single server is overwhelmed, enhancing availability and scalability.</li>
    <li><strong>Frontend Servers:</strong> These servers handle user-facing operations such as logging in, submitting questionnaires, and interacting with the user dashboard. They route requests to the appropriate backend services for further processing.</li>
</ul>

<h2>2. Backend</h2>
<h3>Components:</h3>
<ul>
    <li><strong>User Authentication Service:</strong> Manages user login processes and session management. It ensures secure access to the application by verifying user credentials and maintaining session states.</li>
    <li><strong>Questionnaire Service:</strong> Handles the submission and processing of user responses to the questionnaire. Once data is collected, it is stored in the sharded database, linked to the corresponding user ID.</li>
    <li><strong>LLM API Load Balancer:</strong> Distributes requests for script generation across multiple instances of the LLM service. This ensures that the system can handle a high volume of requests simultaneously, distributing the computational load evenly.</li>
    <li><strong>LLM Services:</strong> These services generate personalized storytelling scripts based on user responses. The load balancer ensures that requests are efficiently distributed across multiple LLM service instances.</li>
    <li><strong>Video API Load Balancer:</strong> Manages the distribution of video generation tasks across multiple instances of the video generative service. This load balancer is crucial for handling the resource-intensive task of video creation.</li>
    <li><strong>Video Generative Services:</strong> These services generate videos from the scripts produced by the LLM. The generated videos are large and stored in cloud storage, with references kept in the main database.</li>
</ul>

<h2>3. Main Database (Sharded by User ID)</h2>
<h3>Components:</h3>
<ul>
    <li><strong>Sharded Database:</strong> The main database is horizontally partitioned (sharded) based on user IDs. This means all data related to a specific user (including user information, questionnaire answers, scripts, and video references) is stored in the same shard, optimizing performance and simplifying data access.</li>
    <li><strong>Shard 1:</strong> Manages data for user IDs in the range 1-1000.</li>
    <li><strong>Shard 2:</strong> Manages data for user IDs in the range 1001-2000.</li>
    <li><strong>Shard N:</strong> Extends this structure to cover additional ranges of user IDs as the user base grows.</li>
    <li><strong>Replication:</strong> Each shard is replicated to ensure high availability and fault tolerance. Replication helps maintain data consistency and provides a backup in case of shard failure.</li>
</ul>

<h2>4. Cloud Storage (AWS S3 - not decided)</h2>
<h3>Components:</h3>
<ul>
    <li><strong>Storage Service (AWS S3):</strong> This service is responsible for storing large video files generated by the video generative service. AWS S3 is chosen for its scalability, durability, and cost-effectiveness.</li>
    <li><strong>Video Files:</strong> The actual video content generated from the LLM scripts.</li>
    <li><strong>Database Reference:</strong> References to these video files are stored in the main database shards, ensuring that all related user data can be easily retrieved and displayed.</li>
</ul>

<h2>5. User Interface (User Dashboard)</h2>
<h3>Components:</h3>
<ul>
    <li><strong>User Dashboard:</strong> The central interface where users can view their questionnaire responses, generated scripts, and videos. The dashboard retrieves data from the userâ€™s specific shard in the database and video files from AWS S3.</li>
</ul>

<h2>Data Flow Overview</h2>
<ul>
    <li><strong>User Interaction:</strong> Users access the system via the frontend, where they can log in, submit their questionnaire responses, and view their personalized content.</li>
    <li><strong>Data Processing:</strong> User data is processed by the backend services. The questionnaire service stores responses in the sharded database. The LLM service generates scripts, which are also stored in the database. The video generative service creates videos from these scripts, stored in AWS S3 with references in the database.</li>
    <li><strong>Data Storage:</strong> All data is stored in the sharded database, ensuring efficient access based on user ID. Video content is stored in AWS S3 to handle the large file sizes efficiently.</li>
    <li><strong>Data Retrieval:</strong> The user dashboard pulls data from both the sharded database and AWS S3, presenting it to the user in a seamless, integrated interface.</li>
</ul>

<h2>Scalability and Fault Tolerance</h2>
<ul>
    <li><strong>Load Balancing:</strong> Both the frontend and backend services use load balancers to ensure that incoming requests are distributed efficiently across available servers. This setup improves system reliability and ensures that services can scale to handle increased traffic.</li>
    <li><strong>Sharding:</strong> The database is sharded by user ID, which allows for horizontal scaling. As the number of users grows, new shards can be added to distribute the load.</li>
    <li><strong>Replication:</strong> Each database shard is replicated to provide redundancy and improve fault tolerance. If a primary shard fails, its replica can take over, minimizing downtime.</li>
</ul>
</body>
</html>

